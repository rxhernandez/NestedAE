{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mannodiarun/perovs_dft_ml/blob/main/Neural_Network_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npnk8NNGSTOK"
   },
   "source": [
    "## *Using neural networks to predict perovskite bandgaps*\n",
    "\n",
    "In this tutorial we will learn how to use the [Keras](https://keras.io/) and [Tensorflow](https://www.tensorflow.org/) libraries to create a neural network regression model that estimates perovskite bandgaps.\n",
    "\n",
    "You can find another example of neural network regression using Keras in the [TensorFlow Tutorials](https://nanohub.org/tools/tftutorials) nanoHUB tool.\n",
    "\n",
    "This tutorial uses Python, some familiarity with programming would be beneficial but is not required. Run each code cell in order by hitting \"Shift + Enter\", or clicking the \"Run\" button in the menu bar above this notebook. Feel free to modify the code to familiarize yourself with how the code works.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Import libraries\n",
    "2. Getting data\n",
    "3. Preprocessing and Organizing Data\n",
    "4. Creating the neural network model\n",
    "5. Training the model\n",
    "6. Evaluate model training\n",
    "7. Make some predictions\n",
    "\n",
    "**Get started:** Hit \"Shift-Enter\" on the code cells to run! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAB_FpIMSpeZ"
   },
   "source": [
    "### Step 1. Import libraries\n",
    "\n",
    "We first import the relevant libraries. These imports are over four cells:\n",
    "\n",
    "The first cell imports the [Pandas](https://pandas.pydata.org/) and [Numpy](https://numpy.org/) libraries that we will use to import and convert the data to appropriate formats for the neural network. We will also import modules from the [Scikit-Learn](https://scikit-learn.org/stable/) library that will help pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uwJRHqNFSiFu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjzpiRf9S9IQ"
   },
   "source": [
    "The next cell imports the [Keras](https://keras.io/) and [Tensorflow](https://www.tensorflow.org/) libraries, which we use to construct and train the neural network. The third cell sets the random seed to ensure consistent results every time the notebook is run, an important step in reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7M5w0ig0S7ye"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zdPOo9O_TBUw"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmurnfX2bkNV"
   },
   "source": [
    "### Step 2. Getting data\n",
    "\n",
    "We will download the data from [this](https://github.com/mannodiarun/perovs_dft_ml) Github repository, after which we will use the Pandas `read_csv()` function to read in the data into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMMzU7bvRxyy",
    "outputId": "2a39f4d7-f57b-413c-e339-65edbbc4c7e2"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mannodiarun/perovs_dft_ml/main/HSE_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9U-dKdNwTF8G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Index                       Formula  Decomp   Gap  SLME     K     Rb  \\\n",
      "0       85                        KSnI_3    2.34  0.49  0.16  1.00  0.000   \n",
      "1       67                       RbSnI_3    1.86  0.53  0.16  0.00  1.000   \n",
      "2      288         RbGe_0.875Sn_0.125I_3    1.96  0.72  0.16  0.00  1.000   \n",
      "3       15                       CsSnI_3   -0.68  0.61  0.16  0.00  0.000   \n",
      "4       82                        KGeI_3    2.41  0.83  0.16  1.00  0.000   \n",
      "..     ...                           ...     ...   ...   ...   ...    ...   \n",
      "239    185            Rb_0.5MA_0.5CaCl_3   -0.13  6.41  0.00  0.00  0.500   \n",
      "240    183     K_0.25Rb_0.25MA_0.5CaCl_3    0.54  6.41  0.00  0.25  0.250   \n",
      "241    184  Rb_0.375Cs_0.125MA_0.5CaCl_3    0.05  6.44  0.00  0.00  0.375   \n",
      "242     45                      MACaCl_3   -0.10  6.63  0.00  0.00  0.000   \n",
      "243     54                      CsCaCl_3    1.27  6.67  0.00  0.00  0.000   \n",
      "\n",
      "        Cs   MA   FA  ...  X_EA  X_IE  X_hof  X_hov  X_En  X_at_num  X_period  \\\n",
      "0    0.000  0.0  0.0  ...   295  1010   7.76   20.9  2.66      53.0       5.0   \n",
      "1    0.000  0.0  0.0  ...   295  1010   7.76   20.9  2.66      53.0       5.0   \n",
      "2    0.000  0.0  0.0  ...   295  1010   7.76   20.9  2.66      53.0       5.0   \n",
      "3    1.000  0.0  0.0  ...   295  1010   7.76   20.9  2.66      53.0       5.0   \n",
      "4    0.000  0.0  0.0  ...   295  1010   7.76   20.9  2.66      53.0       5.0   \n",
      "..     ...  ...  ...  ...   ...   ...    ...    ...   ...       ...       ...   \n",
      "239  0.000  0.5  0.0  ...   349  1250   3.21   10.2  3.16      17.0       3.0   \n",
      "240  0.000  0.5  0.0  ...   349  1250   3.21   10.2  3.16      17.0       3.0   \n",
      "241  0.125  0.5  0.0  ...   349  1250   3.21   10.2  3.16      17.0       3.0   \n",
      "242  0.000  1.0  0.0  ...   349  1250   3.21   10.2  3.16      17.0       3.0   \n",
      "243  1.000  0.0  0.0  ...   349  1250   3.21   10.2  3.16      17.0       3.0   \n",
      "\n",
      "     X_electric_dipole_polarizability_neutral_atom  Tolerance Factor  \\\n",
      "0                                             32.9          0.783094   \n",
      "1                                             32.9          0.804202   \n",
      "2                                             32.9          0.867658   \n",
      "3                                             32.9          0.831642   \n",
      "4                                             32.9          0.854517   \n",
      "..                                             ...               ...   \n",
      "239                                           14.6          0.931066   \n",
      "240                                           14.6          0.923517   \n",
      "241                                           14.6          0.936099   \n",
      "242                                           14.6          1.001525   \n",
      "243                                           14.6          0.893320   \n",
      "\n",
      "     Octahedral Factor  \n",
      "0             0.522727  \n",
      "1             0.522727  \n",
      "2             0.411364  \n",
      "3             0.522727  \n",
      "4             0.395455  \n",
      "..                 ...  \n",
      "239           0.552486  \n",
      "240           0.552486  \n",
      "241           0.552486  \n",
      "242           0.552486  \n",
      "243           0.552486  \n",
      "\n",
      "[244 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"HSE_data.csv\", header=0, skiprows=0)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZJbrNIcTI5F"
   },
   "source": [
    "The first element in each row of this dataframe is a chemical composition, indicated by the column \"Formula\". We need to convert the raw chemical formula into a numerical representation to develop ML models. This process is generally called featurization, and we could use libraries such as [matminer](https://github.com/hackingmaterials/matminer) do to this. In our case, the featurization has already been performed and stored in columns of the CSV file we are using. So we will just load these features into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mwWMkEsJaBQv"
   },
   "outputs": [],
   "source": [
    "#descriptors indicating composition (formula)\n",
    "Comp_desc = pd.DataFrame(data, columns=['K', 'Rb', 'Cs', 'MA', 'FA', 'Ca', 'Sr', 'Ba', 'Ge', 'Sn', 'Pb', 'Cl', 'Br', 'I'])\n",
    "#descriptors using elemental properties (ionic radii, density etc.)\n",
    "Elem_desc = pd.DataFrame(data, columns=['A_ion_rad', 'A_BP', 'A_MP', 'A_dens', 'A_at_wt', 'A_EA', 'A_IE', 'A_hof', 'A_hov', 'A_En', \n",
    "                                        'A_at_num', 'A_period', 'B_ion_rad', 'B_BP', 'B_MP', 'B_dens', 'B_at_wt', 'B_EA', 'B_IE', 'B_hof', \n",
    "                                        'B_hov', 'B_En', 'B_at_num', 'B_period', 'X_ion_rad', 'X_BP', 'X_MP', 'X_dens', 'X_at_wt', \n",
    "                                        'X_EA', 'X_IE', 'X_hof', 'X_hov', 'X_En', 'X_at_num', 'X_period'])\n",
    "#combined descriptors\n",
    "All_desc = pd.DataFrame(data, columns=['K', 'Rb', 'Cs', 'MA', 'FA', 'Ca', 'Sr', 'Ba', 'Ge', 'Sn', 'Pb', 'Cl', 'Br', 'I', 'A_ion_rad', \n",
    "                                       'A_BP', 'A_MP', 'A_dens', 'A_at_wt', 'A_EA', 'A_IE', 'A_hof', 'A_hov', 'A_En', 'A_at_num', 'A_period', \n",
    "                                       'B_ion_rad', 'B_BP', 'B_MP', 'B_dens', 'B_at_wt', 'B_EA', 'B_IE', 'B_hof', 'B_hov', 'B_En', 'B_at_num', \n",
    "                                       'B_period', 'X_ion_rad', 'X_BP', 'X_MP', 'X_dens', 'X_at_wt', 'X_EA', 'X_IE', 'X_hof', 'X_hov', 'X_En', \n",
    "                                       'X_at_num', 'X_period'])\n",
    "\n",
    "X = copy.deepcopy(Elem_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3XjaXFcdZeQ"
   },
   "source": [
    "#### Data Visualization\n",
    "\n",
    "Before training models on our data, we can visualize the kind of data we are working with. This can be done in multiple ways, and in this notebook we are going to look at the range of bandgaps present in our dataset via a histogram.\n",
    "\n",
    "Visualization here is performed using Matplotlib, a Python plotting library. Other options include [Plotly](https://plotly.com/python/) or [Bokeh](https://bokeh.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "av0a71ZOT3cV",
    "outputId": "0b79673e-91a6-4a2c-a1e6-34087e143b84",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.subplots( figsize=(6,6) )\n",
    "plt.subplots_adjust(left=0.11, bottom=0.24, right=0.96, top=0.86, wspace=0.30, hspace=0.40)\n",
    "prop = data.Gap\n",
    "\n",
    "prop_group1 = data.loc[(data.MA > 0) | (data.FA > 0)].Gap\n",
    "\n",
    "plt.hist(prop, color='b', label='_nolegend_')\n",
    "plt.hist(prop_group1, color='r', label='_nolegend_')\n",
    "plt.hist(prop[0], color='b', label='Inorganic')\n",
    "plt.hist(prop_group1[0], color='r', label='Hybrid')\n",
    "plt.rc('xtick', labelsize=24)\n",
    "plt.rc('ytick', labelsize=24)\n",
    "plt.xlabel('Band Gap (eV)', c='k', fontsize=20)\n",
    "plt.legend(loc='upper right', ncol=1, frameon=True, prop={'size':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-PTLvs4TpBa"
   },
   "source": [
    "### Step 3. Preprocessing and Organizing Data\n",
    "\n",
    "We could use all the data at hand to train our model. However, this model would have no guarantee of performing well on an unseen dataset. To mitigate this, we could take one extra step and partition out a \"testing\" dataset from our data. This dataset will not be used in training the model, and will only be used to judge model performance on unseen data. \n",
    "\n",
    "We can train a model using a single training set, and evaluate it on the test set. But many ML models such as neural networks come with \"hyperparameters\" that we do not know the setting for (example: number of layers in a neural network). With one training and testing dataset, we do not have a way to tune the \"hyperparameters\" of the model. \n",
    "\n",
    "There are two solutions to this problem:\n",
    "1. Carve out an additional \"validation\" dataset from the training dataset\n",
    "2. Cross-validation\n",
    "\n",
    "In approach 1, we define a validation set (from within our training set) that will be used as data to evaluate the hyperparameter settings of our model. However, this reduces the amount of data available for training and can be problematic for small datasets. Cross-validation (approach 2) resolves this by using multiple iterations where each iteration uses one chunk of the training data as a validation set. This allows us to tune hyperparameters without wasting too much data.\n",
    "\n",
    "You can read more about cross-validation [here](https://scikit-learn.org/stable/modules/cross_validation.html). \n",
    "\n",
    "Here, we adopt approach 1, i.e, carving a fixed validation set from the training data. For an example of cross-validation, refer to the Random Forest Regressor approach in notebook 2 of this tutorial series.\n",
    "\n",
    "We use the Scikit Learn `train_test_split` function to reserve 20% of the data as a test set. You can read more about the `train_test_split` [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkrEXDuSj3T4"
   },
   "source": [
    "now we need to preprocess the data, we will use a simple min-max normalization scheme from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OVFygneqr5b3"
   },
   "outputs": [],
   "source": [
    "#scale inputs\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "np.savetxt('X_std.csv', X, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8LfexSAETsWR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41756007  0.73256636  0.24166584 ... -1.2521145   1.1805081\n",
      "   1.1803496 ]\n",
      " [-0.7910797   0.7838366   0.4154684  ... -1.2521145   1.1805081\n",
      "   1.1803496 ]\n",
      " [-1.0784025   0.9919335   0.794674   ...  1.2443388  -1.346686\n",
      "  -1.3468994 ]\n",
      " ...\n",
      " [ 0.8179279  -1.309196   -1.6701622  ...  1.2443388  -1.346686\n",
      "  -1.3468994 ]\n",
      " [-0.7910797   0.7838366   0.4154684  ...  0.2457574  -0.08308899\n",
      "  -0.08327492]\n",
      " [-0.41756007  0.73256636  0.24166584 ... -1.2521145   1.1805081\n",
      "   1.1803496 ]]\n",
      "[0.93 2.83 4.61 3.3  4.07 4.6  1.24 3.29 2.   2.86 4.08 1.72 3.96 2.48\n",
      " 4.12 5.56 6.24 5.19 6.31 1.17 2.25 4.57 1.9  0.83 2.89 1.27 2.56 1.53\n",
      " 1.63 1.54 2.59 2.26 5.61 1.79 0.89 0.61 2.86 1.27 1.65 5.47 2.56 2.34\n",
      " 4.38 5.41 1.35 4.21 2.83 2.39 6.45 1.44 1.7  2.42 5.01 3.21 2.52 0.97\n",
      " 3.2  4.27 3.66 1.91 1.1  1.56 1.41 5.63 0.74 1.34 1.8  3.96 6.41 4.1\n",
      " 4.24 2.36 4.41 4.07 0.72 1.62 3.95 5.73 1.87 4.75 4.1  3.29 2.23 4.64\n",
      " 3.19 4.1  2.81 4.06 6.22 2.36 0.49 1.21 2.87 2.26 1.42 1.14 3.15 6.44\n",
      " 3.98 1.49 5.47 3.19 3.   3.32 3.26 4.78 6.63 0.87 1.43 2.23 3.08 5.91\n",
      " 4.51 4.65 5.02 4.46 3.22 5.39 0.84 2.85 0.65 0.9  1.98 3.92 2.32 0.53\n",
      " 3.4  4.89 1.22 3.57 3.89 6.22 0.82 1.98 3.52 1.23 4.15 4.57 3.65 2.01\n",
      " 1.4  4.59 3.36 2.76 3.84 1.02 4.21 3.88 4.51 6.04 5.88 1.58 3.92 3.93\n",
      " 5.14 3.91 2.   4.36 1.8  2.28 6.41 4.01 2.54 4.52 4.67 1.89 3.98 6.3\n",
      " 1.84 1.36 1.34 4.14 2.1  6.02 4.47 5.89 1.88 4.67 1.32 3.36 2.98 5.58\n",
      " 2.98 1.6  0.91 1.94 5.29 0.91 2.11 4.67 2.12 4.75 3.61 1.39 4.25]\n",
      "[[-0.41756007  0.73256636  0.24166584 ...  1.2443388  -1.346686\n",
      "  -1.3468994 ]\n",
      " [-0.41756007  0.73256636  0.24166584 ... -1.2521145   1.1805081\n",
      "   1.1803496 ]\n",
      " [-0.41756007  0.73256636  0.24166584 ...  1.2443388  -1.346686\n",
      "  -1.3468994 ]\n",
      " ...\n",
      " [ 1.8522899  -1.1523694   1.063278   ...  0.2457574  -0.08308899\n",
      "  -0.08327492]\n",
      " [-1.0784025   0.9919335   0.794674   ... -1.2521145   1.1805081\n",
      "   1.1803496 ]\n",
      " [ 0.8179279  -1.309196   -1.6701622  ...  1.2443388  -1.346686\n",
      "  -1.3468994 ]]\n",
      "[1.89 2.29 2.51 3.51 5.72 1.45 1.95 4.4  4.71 4.38 2.74 1.69 3.97 4.92\n",
      " 5.76 2.38 3.96 1.99 6.67 4.71 5.3  3.23 3.82 0.79 4.95 5.01 5.49 3.54\n",
      " 0.73 1.22 1.89 2.02 0.86 4.19 1.37 4.26 0.94 3.75 5.28 5.68 5.58 3.45\n",
      " 6.17 4.   5.41 1.27 1.87 1.16 2.7 ]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, data.Gap, test_size=0.2, random_state=0)\n",
    "\n",
    "ntrain = Y_train.size\n",
    "ntest = Y_test.size\n",
    "\n",
    "X_train_fl = np.array(X_train, dtype=\"float32\")\n",
    "Y_train_fl = np.array(Y_train, dtype=\"float32\")\n",
    "\n",
    "X_test_fl = np.array(X_test, dtype=\"float32\")\n",
    "Y_test_fl = np.array(Y_test, dtype=\"float32\")\n",
    "\n",
    "print(X_train_fl)\n",
    "print(Y_train_fl)\n",
    "print(X_test_fl)\n",
    "print(Y_test_fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmGPMeHjTufI"
   },
   "source": [
    "We now further divide the non-test dataset into training and validation sets. The first 80% is reserved for training, and the remaining 20% is reserved as a validation set. A validation set helps us control model hyperparameters, and monitor for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-kKw1cXnTwea"
   },
   "outputs": [],
   "source": [
    "#prepare training and validation sets\n",
    "X = X_train_fl\n",
    "Y = Y_train_fl\n",
    "\n",
    "idx = int(0.8*X.shape[0]) #Get a validation set\n",
    "Xtrain = X[:idx, :] #80% training\n",
    "Ytrain = Y[:idx].reshape((-1, 1))\n",
    "Xval = X[idx:, :] #20% validation\n",
    "Yval = Y[idx:].reshape((-1, 1))\n",
    "\n",
    "#prepare testing set\n",
    "Xtest = X_test_fl\n",
    "Ytest = Y_test_fl.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUwTnECBTyqT",
    "outputId": "93ea596f-4d84-4b2b-e433-8d6da0147a43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156, 36) (156, 1) (39, 36) (39, 1) (49, 36) (49, 1)\n"
     ]
    }
   ],
   "source": [
    "#check for consistency\n",
    "print (Xtrain.shape, Ytrain.shape, Xval.shape, Yval.shape, Xtest.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdSCyHkLT0xh"
   },
   "source": [
    "### Step 4. Creating the Neural Network\n",
    "\n",
    "We define our neural network architecture in terms of two primary quantities, the number of layers in the network, and the number of \"neurons\" or nodes in each layer. In this case, we will use two hidden layers, each with 100 nodes. To specify this architecture, we will use the Keras Sequential class.\n",
    "\n",
    "\n",
    "The layers use [Tanh](https://en.wikipedia.org/wiki/Hyperbolic_functions) (Hyperbolic Tangent) and [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) (Rectified Linear Unit) [activation functions](https://en.wikipedia.org/wiki/Activation_function). Click [here](https://towardsdatascience.com/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046) to see and visualize the equations describing these activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "p_Cjey4lT30J"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential() #initialize a Sequential model\n",
    "model.add(keras.Input(shape=(36,))) #Add an input layer, the shape parameter tells how many inputs each data point will have\n",
    "model.add(keras.layers.Dense(100, activation='tanh')) #Dense defines a fully connected layer, the argument specifies the number of neurons\n",
    "model.add(keras.layers.Dense(100, activation='tanh')) #activation defines the activation function applied after each layer\n",
    "model.add(keras.layers.Dense(1, activation='relu')) #Output layer can use a 'relu' activation since outputs are always positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oiAh2Q4T69P",
    "outputId": "e0b3ba14-a8a4-4693-851e-ad7abe069e7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               3700      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,901\n",
      "Trainable params: 13,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#Summary helps view the layers in the model\n",
    "#and the number of parameters in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc3lk9TWT9HU"
   },
   "source": [
    "Before the model is ready for training, we need to specify a few more settings. These are added during the model's compile step:\n",
    "\n",
    "- *Loss function:* This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction. Here we use the mean squared error loss function. Click [here](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23) to learn more about loss functions.\n",
    "- *Optimizer:* This decides the optimization technique used to achieve a minimum for the loss function. Here we use the [Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam), click [here](https://climin.readthedocs.io/en/latest/adam.html) to learn more.\n",
    "- *Epochs:* This decides how long to train the model. One epoch is defined as one iteration over the entire training set, where each iteration loops over all sample batches from the training set. Click [here](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9) to learn more about iterations, epochs and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RZoOJUKEUAIs"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Initialize an Adam optimizer with a learning rate of 0.001\n",
    "model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError()) #Compile the model with the Adam optimizer and MSE loss\n",
    "EPOCHS = 100 #Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjREc2gmUDtT"
   },
   "source": [
    "### Step 5. Train the model\n",
    "\n",
    "We can now train the model using the `model.fit()` function. We pass our training data `(Xtrain, Ytrain)` and the validation data `(Xval, Yval)`, which we can use to check whether our model is overfitting.\n",
    "\n",
    "We train our model for 100 epochs. An epoch is when the model has passed through the entire dataset once. Note that this is different from iterations, because each iteration loops over a \"batch\" of data, where a \"batch\" typically consists of a small subsset of our dataset. We use batches to speed up training, especially for large datasets. For more information on epochs, batches, and iterations, check [this](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9) resource.\n",
    "\n",
    "Keras automatically handles backpropogation and updating model weights. To learn more about backpropagation and how neural networks learn, you can click [here](https://www.youtube.com/watch?v=aircAruvnKk) or [here](https://www.youtube.com/watch?v=Ilg3gGewQ5U). It is possible customize this process when engineering advanced networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[2].weights[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OO89aC1iUGUR",
    "outputId": "e3805ebe-0b11-4cea-910d-12aeafaa982a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 17:27:38.067047: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 19ms/step - loss: 10.2984 - val_loss: 9.1370\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 7.5235 - val_loss: 7.3593\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 7.0018 - val_loss: 6.6272\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 6.2799 - val_loss: 5.6922\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.8091 - val_loss: 5.1451\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 3.8572 - val_loss: 4.0857\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 3.2951 - val_loss: 3.2201\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.9416 - val_loss: 2.6544\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.7758 - val_loss: 2.5586\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.5524 - val_loss: 2.6226\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.3494 - val_loss: 2.6795\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.1844 - val_loss: 2.5801\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.9876 - val_loss: 2.3937\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.8983 - val_loss: 2.2201\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.8338 - val_loss: 2.1371\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.7600 - val_loss: 2.0875\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.6954 - val_loss: 2.0230\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.6648 - val_loss: 1.9522\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.6347 - val_loss: 1.8889\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.6053 - val_loss: 1.8378\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.5535 - val_loss: 1.7964\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.5117 - val_loss: 1.7405\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.4394 - val_loss: 1.6929\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.3565 - val_loss: 1.6250\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.2768 - val_loss: 1.5015\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.1689 - val_loss: 1.4086\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.0740 - val_loss: 1.2699\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.9833 - val_loss: 1.1296\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.9084 - val_loss: 1.0288\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.8492 - val_loss: 0.9369\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.7762 - val_loss: 0.7826\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.7146 - val_loss: 0.7437\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.6203 - val_loss: 0.6416\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5776 - val_loss: 0.5671\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5414 - val_loss: 0.5641\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5140 - val_loss: 0.5268\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4854 - val_loss: 0.4922\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4615 - val_loss: 0.4971\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4433 - val_loss: 0.4439\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4253 - val_loss: 0.3894\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3904 - val_loss: 0.3808\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3397 - val_loss: 0.3169\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2800 - val_loss: 0.2643\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2274 - val_loss: 0.1780\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1550 - val_loss: 0.1009\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1346 - val_loss: 0.0819\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1221 - val_loss: 0.0911\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1195 - val_loss: 0.0686\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1131 - val_loss: 0.0566\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1116 - val_loss: 0.0510\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1031 - val_loss: 0.0684\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1015 - val_loss: 0.0529\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0973 - val_loss: 0.0540\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0935 - val_loss: 0.0626\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 0.0581\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 0.0627\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0935 - val_loss: 0.0512\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0870 - val_loss: 0.0607\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0860 - val_loss: 0.0570\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0861 - val_loss: 0.0487\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0933 - val_loss: 0.0538\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0837 - val_loss: 0.0639\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0835 - val_loss: 0.0558\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 0.0578\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0791 - val_loss: 0.0525\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 0.0573\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.0559\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 0.0621\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0746 - val_loss: 0.0520\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0755 - val_loss: 0.0568\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0598\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 0.0554\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0750 - val_loss: 0.0572\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 0.0564\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0749 - val_loss: 0.0595\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0622\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0704 - val_loss: 0.0669\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0694 - val_loss: 0.0534\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0667 - val_loss: 0.0588\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0662 - val_loss: 0.0602\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.0565\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0650 - val_loss: 0.0577\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.0573\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.0572\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.0562\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.0582\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.0576\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.0584\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.0631\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.0576\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.0572\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0593 - val_loss: 0.0626\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0619 - val_loss: 0.0604\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.0676\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.0638\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.0632\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.0600\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.0620\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.0615\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0575 - val_loss: 0.0607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x280ace970>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, Ytrain, epochs=100, validation_data=(Xval, Yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QftWMfaUTQV"
   },
   "source": [
    "At this point, we can check some of the [weights](https://en.wikipedia.org/wiki/Synaptic_weight) from the trained neural network. These weights, in a way, represent the relationship between inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVA4UkiSUZCM",
    "outputId": "7f7d5764-6ef0-4947-ea53-fb74016bfc57"
   },
   "outputs": [],
   "source": [
    "weights = model.layers[-1].get_weights()[0]\n",
    "weights[3] #prints the 3rd weight in the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQrltlhhUaSq"
   },
   "outputs": [],
   "source": [
    "#The history object contains the training and validation losses, which we can plot\n",
    "training_loss = model.history.history['loss']\n",
    "validation_loss = model.history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhLODCPpUc5k"
   },
   "source": [
    "### Step 6. Evaluate model training\n",
    "\n",
    "We use pyplot from [Matplotlib](https://matplotlib.org/) to plot the \"Learning Curve\", which is a plot that shows the evolution of training and validation loss over epochs. We expect the training and validation losses to go down if the training went well. More importantly, if the validation loss goes up after a while, we know that the model is overfitting. In Keras, this overfitting can be prevented using the `EarlyStopping()` functionality, click [here](https://keras.io/api/callbacks/early_stopping/) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "tIgwtMcyUfIG",
    "outputId": "ec26a076-8c48-4852-c93c-23215d785aea"
   },
   "outputs": [],
   "source": [
    "plt.plot(training_loss, color='blue', label='training', lw=3)\n",
    "plt.plot(validation_loss, c='orange', label='validation', lw=3)\n",
    "plt.xlabel('Epoch', c='k', fontsize=20)\n",
    "plt.ylabel(\"Loss value (eV$^2$)\", c='k', fontsize=20)\n",
    "plt.rc('xtick', labelsize=24)\n",
    "plt.rc('ytick', labelsize=24)\n",
    "plt.legend(loc='upper right', ncol=1, frameon=True, prop={'size':20})\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ByFTdVMUhGx"
   },
   "source": [
    "At this stage, if we are satisfied with the model training, we can call the `model.evaluate()` function on the training, validation, and test sets to measure model performance. If we wish to improve training, we would go back and change parameters in the building of the model such as running the training for more epochs or adding more layers to the model. The effect of these changes will be tracked by the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhrJvOanUjFf",
    "outputId": "469681b4-8029-44d3-80cb-67d7585e7052"
   },
   "outputs": [],
   "source": [
    "#The model.evaluate() function evaluates the model on the training, validation and testing datasets\n",
    "mse_train = model.evaluate(Xtrain, Ytrain)\n",
    "mse_val = model.evaluate(Xval, Yval)\n",
    "mse_test = model.evaluate(Xtest, Ytest)\n",
    "\n",
    "print(f\"Averaged Squared Error For each Data Partition:\\nTrain:\\t\\t{mse_train}\\nValidation:\\t{mse_val}\\nTest:\\t\\t{mse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_9bOdNIUjAW"
   },
   "source": [
    "We can save the model into an h5 format training by using the `model.save()` function. This saved model can be reloaded using the `load_model()` function.\n",
    "\n",
    "This tool is running in a read only filesystem, so this code is just for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jFRyLbDUpcD"
   },
   "source": [
    "model.save('./Models/nn_c_bg.h5')\n",
    "\n",
    "load_model = keras.models.load_model('./Models/nn_c_bg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Un6nY3oUues"
   },
   "source": [
    "### Step 7. Make some predictions\n",
    "\n",
    "We can call the `model.predict()` function to make predictions. We will use this function to make predictions on the train, validation and test sets. We expect good predictions for the training and validation sets, but the predictions on the test sets are unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrqgEWAgUx8d",
    "outputId": "b2fe6e63-fab3-4393-9c42-063651bb328c"
   },
   "outputs": [],
   "source": [
    "Y_pred_tr = model.predict(Xtrain)\n",
    "Y_pred_val = model.predict(Xval)\n",
    "Y_pred_test = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssx3cfy3Uz58"
   },
   "source": [
    "### Step 8. Visualize results\n",
    "\n",
    "We are now ready to visualize the results of our training. One way to do this is to plot the model predictions compared to the ground truth data, known as a parity plot. We see that the model does a reasonable job at predicting band gaps for each of the train, validation, and test sets. Note that in a real world application, we will not know the ground truth values for the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "8A3wQ00fU2kX",
    "outputId": "6d8ad619-10c9-4723-96c5-46d3505e84a5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(Ytrain, Y_pred_tr, 'ro', label=\"training\")\n",
    "plt.plot(Yval, Y_pred_val, 'bo', label=\"validation\")\n",
    "plt.plot(Ytest, Y_pred_test, 'go', label=\"testing\")\n",
    "plt.plot(Ytrain, Ytrain, 'k-')\n",
    "plt.gca().set_aspect(1.0)\n",
    "plt.xlabel('Ground truth bandgap (eV)', c='k', fontsize=16)\n",
    "plt.ylabel(\"Predicted bandgap (eV)\", c='k', fontsize=16)\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yUFDYTywrPU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
